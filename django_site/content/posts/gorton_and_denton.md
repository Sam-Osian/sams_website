---
draft: true
id: 2
title: What the Gorton and Denton by-election reveals about how political statistics are used
slug: gorton-and-denton
authors:
  - sam
date: 2026-02-21
categories:
  - Opinion
  - Statistics
meta:
  - property: og:image
    content: /assets/images/gorton_denton.png
---

In theory, polling sits on the sober end of politics. It is meant to measure rather than persuade, to describe rather than perform. But the Gorton and Denton by-election has become a small, instructive theatre in how easily that boundary can blur.

![What the Gorton and Denton by-election reveals about how political statistics are used](../assets/images/gorton_denton.png){ width="700" }

<!-- more -->

---

Across the campaign, Labour, the Greens and Reform have all circulated charts, projections and data claims. Some draw on formal polling. Others rely on modelling, internal canvassing or commissioned snapshots. Most are presented with the same visual authority: a clean bar chart, a clear leader, a sense of direction. One Labour graphic, for instance, presented a bar chart to support the message that “only Labour can beat Reform” in the seat, meanwhile a Green graphic pushed a similar “only we can stop Reform” logic using its own chart.

Taken together, they offer less a picture of public opinion than a picture of how statistics are deployed when the stakes are local, tight and highly visible.


## Polling as persuasion

One of the clearest shifts on display is how polling is now used as a campaign instrument rather than simply a diagnostic tool. Numbers are presented not just to understand the race but to shape it. A favourable result becomes evidence of momentum. A close second-place showing becomes an argument for tactical voting. Even ambiguous findings can be framed as proof that support is “coalescing”. 

A Green Party contested graphic is a neat example of this: it framed a projection of national polling as “latest local polling” while pairing it with a tactical claim about being the only barrier to a Reform win.

Reform’s contribution to this isn’t just about what is published, but what is *withheld*. Reform has briefed journalists on [“internal polling”](https://www.newstatesman.com/politics/westminster/inside-westminster/2026/01/labour-is-under-siege-in-gorton-and-denton) that the public can’t see, presenting it as privileged insight into the real state of the race. The detail being trailed is telling in itself: Reform showing themselves to be in first place, while the Greens were in second ahead of Labour.

That move matters because secrecy can work like a force multiplier. A public poll invites scrutiny: who commissioned it, what was the sample, what were the questions? “Internal polling” arrives with fewer handles to grab, and leaking this infromation to the media brings all the luxuries of favourable polling figures with none of this accountability.

The point, as far as Parties across the political spectrum are concerned, isn't necessarily to invent figures. It is to create plausibility. If a party can appear competitive, even marginally so, it can attract attention, volunteers and tactical voters who might otherwise look elsewhere. Statistics, in this sense, function less as a mirror and more as a spotlight.


## The quiet fragility of small samples

Polls feel authoritative because they borrow the language of science: sample sizes, percentages, tidy charts. But a poll is only as strong as the number of people it actually asked, and crucially, the number of people whose answers end up underpinning the claim being made.

This is where things can become fragile without anyone obviously lying. A poll might start with a few hundred respondents, then be filtered down to “likely voters”, or to voters who have already reached a decision, or to a subgroup being used to justify a tactical-voting message. The headline sample size stays in the background like a stamp of credibility, while the effective number doing the real work shrinks.

[Full Fact’s write-up of Labour’s bar chart](https://fullfact.org/politics/labour-gorton-and-denton-by-election-chart/?utm_source=chatgpt.com) is a concrete illustration: the comparison being used to support the “only Labour can beat Reform” line was based on just 62 respondents. That does not mean the result is worthless, but it does mean it is much more uncertain than the chart’s crisp bars suggest.

This is rarely visible in the final graphic. Margins of error, uncertainty ranges, and the “how many people did this actually come from?” question are often absent. To a reader glancing at a leaflet or social media post, a claim built on a few dozen respondents can look as solid as one built on several hundred.

## When projections look like polling

A lot of confusion in modern election graphics comes from one simple mix-up: polling and projections are not the same thing.

A **poll** is, at least in principle, a direct measurement: you ask a sample of people how they would vote, and you report what they said, with caveats.

A **projection** is a modelled estimate: you start with some data (which might include past results, demographic patterns, national trends, assumptions about turnout, or even other polls), then you apply a set of rules to produce a best guess about what might happen. Projections can be helpful, but they are only as good as their assumptions. If you change the assumptions, you can change the output.

The problem is that projections often look like polls once they are turned into a bar chart. They share the same visual grammar: party names, vote shares, a clear “winner”. The distinction between “this is what people told us” and “this is what our model suggests might happen” becomes easy to miss.

In this by-election, the Green example is instructive because the chart drew on an Election Maps UK projection and showed figures like Reform at “+12%”, Greens at “+11%” and Labour at “-20%”, yet it was positioned in a way that encouraged readers to treat it as “latest polling”.


## The appeal of data ‘from the ground’

Internal canvassing figures add a different kind of statistical claim to the mix. Campaigns routinely gather information from doorstep conversations and volunteer reports. This can be genuinely useful for operations: it tells parties where they are hearing enthusiasm, which streets are receptive, and where to send the next batch of volunteers.

But canvassing data is not a survey. It is not designed to represent everyone. It reflects who answers the door, who is willing to engage, and where campaigners choose to concentrate their efforts. A party that mostly knocks on areas where it expects support will “discover” more support. A party with highly motivated volunteers will gather more data from its own strongest pockets.

Yet these numbers can be presented in ways that suggest a broad constituency snapshot. Their persuasive power lies in their immediacy. They feel tangible. They come with the romance of “real conversations”, even when the statistical foundations are narrower than the rhetoric implies.

## Uncertainty reframed

By-elections are notoriously difficult to measure. Turnout is volatile, response rates are low, and local dynamics can shift quickly. In such conditions, the most accurate statistical conclusion is often that the race is uncertain.

But uncertainty travels poorly as a campaign message. So it is translated. A narrow gap becomes a surge. A high proportion of undecided voters becomes latent support waiting to crystallise. The absence of clear evidence is reinterpreted as evidence of change.

You can see how easily this happens by looking at a single constituency poll and imagining the competing storylines. Find Out Now’s late-January poll, for example, put Reform on 36%, Labour on 33% and the Greens on 21% (excluding “don’t know”), based on a sample of 143 residents. Depending on your goals, you can frame that as “Reform ahead”, “Labour within reach”, or “Greens are the only viable challenger if tactical voters unite”. ([findoutnow.co.uk](https://findoutnow.co.uk/blog/gorton-and-denton-by-election-poll/?utm_source=chatgpt.com))

## What this moment illustrates

None of this is unique to one party or one by-election. What makes Gorton and Denton instructive is the way these practices have surfaced so visibly in a single, tightly fought contest. With multiple parties claiming viability and attention at a premium, the incentive to produce compelling numerical stories is unusually strong.

The broader lesson is not that polling has lost its value. It remains one of the few systematic ways to gauge public opinion between elections. But its authority is increasingly mediated by how it is presented, who commissions it, what is being modelled rather than measured, and what gets left out when a graphic is simplified for maximum impact.

Statistics carry weight in politics because they appear objective. That appearance can be borrowed, stretched and occasionally overextended. The Gorton and Denton by-election offers a reminder that numbers rarely arrive untouched by the hands that publish them. They measure something real, but they are also made to work.

In the end, the contest is less a failure of polling than a reflection of its power. When numbers shape perception, every campaign wants a set that speaks in its favour. And when every chart carries an argument, the task for readers is not simply to read the numbers, but to read the intentions behind them.
