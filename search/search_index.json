{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Hi, I'm Sam \ud83d\udc4b</p> <p>I'm a Health Data Scientist and Doctoral Researcher at the University of Liverpool, where I use AI and open data to help uncover and prevent harm in healthcare. </p> <pre><code>Enjoy your stay!\n</code></pre> <p></p>"},{"location":"#recent-posts","title":"Recent posts","text":""},{"location":"about/","title":"About me","text":"<pre><code>Hiya :)\n</code></pre> <p>I\u2019m Sam Osian (pronounced osh-ahn), an AI doctoral researcher currently based in the University of Liverpool. My main focus is currently on understanding what we can learn from coroner inquest data, with the aim of translating evidence into practical tools for researchers, as well as those on the front lines of care.</p> <p>I\u2019m a big believer that data should do more than just sit in a spreadsheet. When it\u2019s open, shared, and actually used, it can prevent harm and make care better. That\u2019s why I love open-source projects and collaborative work: anything that helps turn insight into action.</p>"},{"location":"about/#my-background","title":"My background","text":"<p>Before I got into data science, I actually started out as a social scientist, studying politics at the University of Bristol from 2017-2021. It's far from the typical route into AI (and has definitely previously given me a big dose of imposter syndrome!), but I've learnt to love my background. It's made me a big picture thinker, and probably a bit more sceptical than most about the limits of data &amp; AI. I like seeing how numbers fit into the messiness of real life, and I think that mix is an advantage when it comes to tackling problems that actually matter. </p> <p>My bridge into data science came from from working as a researcher in the charity sector. I routinely used data and insight for service evaluation in organisations like Citizens Advice, UK Youth and Samaritans. As time went on, I became frustrated with the big questions that I couldn't answer, because I didn't have the technical know-how.</p> <p>Following this, I pursued a Master's in Medical Statistics, again at the University of Bristol, in 2023. For each skill I gained during my Master's, I probably also gained a new source of frustration. Specifically, I saw how, in academia, so much analysis was being done for analysis sake, with opportunities for integrating that knowledge few and far between. I wanted to prove that academia could be about more than simply producing a collection of static knowledge for us all to sit around stroking our chins and exclaiming \u201chmm, interesting.\u201d</p> <p>This led me to my PhD at the University of Liverpool, where I now work out of the Mental Health Research for Innovation Centre (M-RIC) and the Civic Health Innovation Lab (CHIL). Under the terrific supervisory team of Dan Joyce and Iain Buchan, I'm now tasked with precisely this problem - contributing towards data-driven solutions that address real challenges.</p>"},{"location":"publications/","title":"Content coming soon \u2728","text":""},{"location":"2025/07/14/rethinking-statistical-significance/","title":"Rethinking 'statistical significance' \u2014 Has the p-value overstayed its welcome?","text":"<p>In research, the p-value is often treated as the ultimate test of truth. But should all statistical analysis be held to the same statistical standard? </p> <p>In this post, I argue that the p-value is too often used as a universal yardstick, regardless of context or consequences. Sometimes, what counts as \u201cenough\u201d evidence depends on the risks and decisions at stake, not just on whether a result falls below an arbitrary threshold.</p> <p></p>"},{"location":"2025/07/14/rethinking-statistical-significance/#explaining-the-p-value","title":"Explaining the p-value","text":"<p>If you're somewhat familiar with law (or have at least watched a courtroom drama... or if you're me, Judge Judy), you'll probably know that there are different standards of proof depending on the type of court. </p> <p>In criminal cases, this bar is set really high. A jury must be sure \u201cbeyond a reasonable doubt\u201d that the defendant did in fact commit the crime. We can (loosely) quantify this as needing to be ~99.5% certain. In civil disputes, it\u2019s \u201con the balance of probabilities\u201d \u2014 essentially, what\u2019s more likely than not (i.e. being at least 51% sure). </p> <p>It makes intuitive sense to say that different situations require different thresholds of confidence for making decisions, depending on what's at stake, and how important the end decision is. In criminal court, we can hardly send someone to jail if the evidence points towards but ultimately does not clearly demonstrate that they did in fact carry out the alleged crime. For civil disputes, where neither party faces criminal conviction, the stakes are much lower.</p> <p>Yet, in statistical research, we often fail to extend this logic. The p-value \u2014 usually pegged at 0.05 \u2014 has become a one-size-fits-all verdict on what counts as evidence. </p> <p>At its simplest, the p-value tells us how likely it is to observe results as extreme as the ones we got, just by random chance, if there was actually no effect at all. The lower the p-value, the less likely it is that our results are a fluke. Below the 0.05 threshold, a result is often deemed \u201cstatistically significant\u201d; above it, often written off as lacking sufficient evidence. </p> <p>The effect of this threshold obscures a much more prudent question: what level of evidence is appropriate for the decision at hand?</p>"},{"location":"2025/07/14/rethinking-statistical-significance/#where-things-start-to-fall-apart","title":"Where things start to fall apart","text":"<p>Consider this hypothetical scenario: a school-based breakfast programme is trialled to see if it improves pupils\u2019 attendance. The study finds a positive effect: among a sample of students with poor baseline attendance, attendance improved by 14% following engagement with the breakfast programme.</p> <p>A statistical researcher plugs these data into a model of some kind, and reports this effect alongside a p-value of 0.11. This misses the conventional threshold of 0.05 and, consequently, the researcher deems the results as \u201cstatistically insignificant.\u201d The school principal, reading the report, decides to drop the breakfast programme entirely. </p> <p>But let\u2019s look at the real-world context. Assume that providing breakfast at school is low-cost, low-risk, and easy to implement. The wider literature may also suggest potential knock-on benefits, such as improved community cohesion. In other words, there\u2019s little to lose, and the evidence \u2014 though not definitive \u2014 certainly suggests a likely benefit. The effect size itself is modest but it's absolutely practically significant. In other words, the programme is likely to make a positive difference to the school.</p> <p>If this were a civil court, we\u2019d simply ask: is it more likely than not that this intervention is associated with improved attendance? On that standard, the answer is a clear yes. The effect size and the p-value, when taken together, cleanly pass the \u201cthe balance of probabilities\u201d threshold. Given the low-cost, low-risk nature of the programme, this seems to be vastly more appropriate than applying a \u201cbeyond reasonable doubt\u201d standard.</p>"},{"location":"2025/07/14/rethinking-statistical-significance/#but-lets-not-go-too-far","title":"...But let's not go too far","text":"<p>Introducing new medication is a completely different story. The risks could be higher (especially if the drug has major side effects), and the consequences more serious. Medical ethics correctly tells us to be vigilant that we're acting in a given patient's best interests. In this case, a much lower p-value \u2014 something closer to \u201cbeyond reasonable doubt\u201d \u2014 is entirely justified and most likely necessary. </p> <p>The point I'm endeavouring to make is that while relying on fixed, uncritical statistical thresholds may be a convenient rule, it's not a particularly useful or thoughtful one. By insisting on a universal threshold, we end up applying the same standard whether the stakes are routine or life-changing. Worse, it encourages us to treat the result as binary: significant or not, yes or no. This doesn't match the grey, murkiness of real-world uncertainty.</p> <p>The broader issue here is that statistical prediction isn\u2019t the same as decision-making. Prediction should inform our choices, but complex decisions require a layer of human judgement to mediate between numbers and action. In this way, the p-value should not be used 'checklist' style but rather interpreted within the context of the research area, as well as related statistical metrics like the point estimate, confidence interval, and measures of dispersion.</p>"},{"location":"2025/07/14/rethinking-statistical-significance/#language-is-important","title":"Language is important","text":"<p>Language plays a part. The phrase \u201cnot significant\u201d is a way of stepping back from judgment, leaving the interpretation to the number itself. If instead we said, \u201cThere is moderate evidence, but uncertainty remains,\u201d we would be more honest \u2014 and probably more useful to anyone trying to make a real-world decision. </p> <p>Most researchers have felt the frustration of working on a promising project, only for a regression model to return a p-value of, say, 0.054. It's frustrating precisely because we know that our results provide moderate evidence towards our research aim, yet they are still \u201cinsignificant\u201d for all intents and purposes. </p> <p>This leads some researchers to adopt awkward, and sometimes quite comical, phrasing. One study researching the potential health impacts of green coffee consumption regularly used the phrase \u201calmost statistically significant\u201d to describe their p-values, which hovered around 0.06. I\u2019m not criticising the researchers here, but it does show that the gravitational pull of the 0.05 threshold is strong \u2014 even when we know, deep down, that reality is rarely so clear-cut.</p> <p></p> <p>A published research paper using the phrase \u2018almost statistically significant\u2019 for p-values just missing the conventional threshold.</p> <p>I do want to be transparent here: everything I've said above isn't a novel critique. Many have already been arguing for a more nuanced approach before, particularly in epidemiology where framing of the p-value in terms of providing \u201clow / moderate / strong / very strong\u201d evidence has already started to take hold. </p> <p>But still, the checkbox culture persists, especially within the social sciences. We can do better by recognising that the appropriate standard of evidence depends on context \u2014 not by pretending that 0.05 has universal applicability.</p> <p>In the end, statistics is a tool for making sense of uncertainty, not erasing it. A p-value is just one part of the story. The real work lies in making thoughtful decisions about what evidence means for the world outside the dataset.</p> <p>Share on  Share on </p>"}]}